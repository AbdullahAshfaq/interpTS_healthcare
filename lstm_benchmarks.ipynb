{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Complexity metrics on all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 22:27:23.569377: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-01 22:27:23.590670: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-01 22:27:23.706948: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-01 22:27:23.707038: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-01 22:27:23.707314: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 22:27:23.775940: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-01 22:27:23.780299: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 22:27:41.095138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/mnt/c/D_drive/UCSD/Quarters/Q4/DSC261-RespDS/ts_causal_wsl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from TSInterpret.InterpretabilityModels.Saliency.TSR import TSR, Saliency_PTY\n",
    "from TSInterpret.InterpretabilityModels.counterfactual.TSEvoCF import TSEvo\n",
    "# from TSInterpret.InterpretabilityModels.counterfactual.SETSCF import SETSCF\n",
    "\n",
    "import torch \n",
    "from XTSCBench.ClassificationModels.CNN_T import ResNetBaseline, UCRDataset,fit\n",
    "from XTSCBench.ClassificationModels.LSTM import LSTM\n",
    "from XTSCBench.CounterfactualEvaluation import CounterfactualEvaluation\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "import sklearn\n",
    "import numpy as np \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset='ECG5000'\n",
    "# dataset = 'ptbxl'\n",
    "dataset='Epilepsy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if dataset in ['ECG5000','ECG200','Epilepsy']:\n",
    "    #For use with CNN reverse Data Dimensions\n",
    "    train_x, train_y, test_x, test_y=UCR_UEA_datasets().load_dataset(dataset)\n",
    "elif dataset=='ptbxl':\n",
    "    train_x = np.load(f'./datasets/ptbxl/x_train.npy')\n",
    "    train_y = np.load(f'./datasets/ptbxl/y_train.npy')\n",
    "    test_x = np.load(f'./datasets/ptbxl/x_test.npy')\n",
    "    test_y = np.load(f'./datasets/ptbxl/y_test.npy')\n",
    "\n",
    "# 1 hot encoding outcomes\n",
    "enc1=sklearn.preprocessing.OneHotEncoder(sparse=False).fit(np.vstack((train_y.reshape(-1,1),test_y.reshape(-1,1))))\n",
    "train_y=enc1.transform(train_y.reshape(-1,1))\n",
    "test_y=enc1.transform(test_y.reshape(-1,1))    \n",
    "\n",
    "n_pred_classes =train_y.shape[1]\n",
    "\n",
    "train_dataset = UCRDataset(train_x.astype(np.float64),train_y.astype(np.int64))\n",
    "test_dataset = UCRDataset(test_x.astype(np.float64),test_y.astype(np.int64))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "hidden_size=10\n",
    "rnn=0.1\n",
    "input_size = train_x.shape[-1] # univariate or multi?\n",
    "\n",
    "model_path = './trained_models'\n",
    "NumTimesteps = train_x.shape[-2]\n",
    "NumFeatures = train_x.shape[-1]\n",
    "\n",
    "# Model saved by this name\n",
    "model_name = f'lstm_{dataset}_h{hidden_size}_drop{rnn}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='Epilepsy':\n",
    "    model_name = 'lstm_Epilepsy_h200_drop0.3_acc58'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lstm_Epilepsy_h200_drop0.3_acc58 successfully loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=200, out_features=4, bias=True)\n",
       "  (rnn): LSTM(3, 200, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model =LSTM(input_size, hidden_size ,n_pred_classes,rnndropout=0.1).to(device) \n",
    "\n",
    "if os.path.isfile(f'./{model_path}/{model_name}'):\n",
    "    model =LSTM(1, 10 ,n_pred_classes,rnndropout=0.1).to('cpu') \n",
    "    model = torch.load(f'./{model_path}/{model_name}')\n",
    "    print(f\"Model {model_name} successfully loaded\")\n",
    "else:\n",
    "    print(\"Model not found. Please train model using training_models.ipynb and provide in this notebook\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y was one Hot Encoded\n"
     ]
    }
   ],
   "source": [
    "from TSInterpret.InterpretabilityModels.counterfactual.TSEvoCF import TSEvo\n",
    "\n",
    "tsevo_exp = TSEvo(model= model,data=(train_x,train_y), mode = 'time',backend='PYT',epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Saliency \n",
    "\n",
    "## Methods\n",
    "# * Gradients (GRAD)\n",
    "# * Integrated Gradients (IG)\n",
    "# * Gradient Shap (GS)\n",
    "# * DeepLift (DL)\n",
    "# * DeepLiftShap (DLS)\n",
    "# * SmoothGrad (SG)\n",
    "# * Shapley Value Sampling(SVS)\n",
    "# * Feature Ablation (FA)\n",
    "# * Occlusion (FO)\n",
    "\n",
    "from TSInterpret.InterpretabilityModels.Saliency.TSR import TSR, Saliency_PTY\n",
    "\n",
    "tsr_GRAD_exp = Saliency_PTY(model, NumTimeSteps=train_x.shape[-2], NumFeatures=train_x.shape[-1], method='GRAD', mode='time', tsr=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr_FA_exp = Saliency_PTY(model, NumTimeSteps=train_x.shape[-2], NumFeatures=train_x.shape[-1], method='FA', mode='time', tsr=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr_FO_exp =  Saliency_PTY(model, NumTimeSteps=train_x.shape[-2], NumFeatures=train_x.shape[-1], method='FO', mode='time', tsr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NativeGuideCF\n",
    "# from TSInterpret.InterpretabilityModels.counterfactual.NativeGuideCF import NativeGuideCF\n",
    "\n",
    "# ng_exp = NativeGuideCF(model,(train_x,train_y), backend='PYT', mode='feat',method='NUN_CF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer =  [\n",
    "    tsevo_exp,\n",
    "    tsr_FA_exp,\n",
    "    tsr_GRAD_exp,\n",
    "    # tsr_FO_exp\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bm=CounterfactualEvaluation(explainer=explainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Target\n",
      "No Target\n"
     ]
    }
   ],
   "source": [
    "SummaryTable = bm.evaluate(test_x[0:2], np.argmax(test_y[0:2],axis=1),model, mode='time',aggregate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1_mean</th>\n",
       "      <th>d2_mean</th>\n",
       "      <th>d3_mean</th>\n",
       "      <th>d4_mean</th>\n",
       "      <th>validty_mean</th>\n",
       "      <th>d1_std</th>\n",
       "      <th>d2_std</th>\n",
       "      <th>d3_std</th>\n",
       "      <th>d4_std</th>\n",
       "      <th>validty_std</th>\n",
       "      <th>method</th>\n",
       "      <th>normalize</th>\n",
       "      <th>tsr</th>\n",
       "      <th>transformer</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998382</td>\n",
       "      <td>0.623900</td>\n",
       "      <td>386.745868</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>10.046387</td>\n",
       "      <td>0.226274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GRAD</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999191</td>\n",
       "      <td>0.877832</td>\n",
       "      <td>805.530702</td>\n",
       "      <td>4.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.044349</td>\n",
       "      <td>79.354777</td>\n",
       "      <td>0.325269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>authentic_opposing_information</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    d1_mean   d2_mean     d3_mean  d4_mean  validty_mean    d1_std    d2_std  \\\n",
       "0  0.998382  0.623900  386.745868     2.60           1.0  0.000000  0.003408   \n",
       "1  0.999191  0.877832  805.530702     4.02           1.0  0.001144  0.044349   \n",
       "\n",
       "      d3_std    d4_std  validty_std method normalize   tsr  \\\n",
       "0  10.046387  0.226274          0.0   GRAD      True  True   \n",
       "1  79.354777  0.325269          0.0    NaN       NaN   NaN   \n",
       "\n",
       "                      transformer epochs  \n",
       "0                             NaN    NaN  \n",
       "1  authentic_opposing_information     30  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SummaryTable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138, 4)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 2\n",
    "interp_folder = './interp_metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm=CounterfactualEvaluation(explainer=explainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Target\n",
      "No Target\n"
     ]
    }
   ],
   "source": [
    "SummaryTable_counterfact = bm.evaluate(test_x[0:num_test_samples], np.argmax(test_y[0:num_test_samples],axis=1),model, mode='time',aggregate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1_mean</th>\n",
       "      <th>d2_mean</th>\n",
       "      <th>d3_mean</th>\n",
       "      <th>d4_mean</th>\n",
       "      <th>validty_mean</th>\n",
       "      <th>d1_std</th>\n",
       "      <th>d2_std</th>\n",
       "      <th>d3_std</th>\n",
       "      <th>d4_std</th>\n",
       "      <th>validty_std</th>\n",
       "      <th>method</th>\n",
       "      <th>normalize</th>\n",
       "      <th>tsr</th>\n",
       "      <th>transformer</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998382</td>\n",
       "      <td>0.623900</td>\n",
       "      <td>386.745868</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>10.046387</td>\n",
       "      <td>0.226274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GRAD</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999191</td>\n",
       "      <td>0.657026</td>\n",
       "      <td>427.236924</td>\n",
       "      <td>2.624741</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>67.733426</td>\n",
       "      <td>0.260842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FA</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998382</td>\n",
       "      <td>0.874167</td>\n",
       "      <td>773.046050</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.040424</td>\n",
       "      <td>67.197135</td>\n",
       "      <td>0.084853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>authentic_opposing_information</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    d1_mean   d2_mean     d3_mean   d4_mean  validty_mean    d1_std    d2_std  \\\n",
       "0  0.998382  0.623900  386.745868  2.600000           1.0  0.000000  0.003408   \n",
       "1  0.999191  0.657026  427.236924  2.624741           1.0  0.001144  0.042900   \n",
       "2  0.998382  0.874167  773.046050  3.550000           1.0  0.002288  0.040424   \n",
       "\n",
       "      d3_std    d4_std  validty_std method normalize   tsr  \\\n",
       "0  10.046387  0.226274          0.0   GRAD      True  True   \n",
       "1  67.733426  0.260842          0.0     FA      True  True   \n",
       "2  67.197135  0.084853          0.0    NaN       NaN   NaN   \n",
       "\n",
       "                      transformer epochs  \n",
       "0                             NaN    NaN  \n",
       "1                             NaN    NaN  \n",
       "2  authentic_opposing_information     30  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SummaryTable_counterfact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SummaryTable_counterfact.to_csv(f\"{interp_folder}/{model_name}_CF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faithfulness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XTSCBench.FaithfulnessEvaluation import FaithfulnessEvaluation\n",
    "bm=FaithfulnessEvaluation(explainer=explainer,mlmodel=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Target\n",
      "No Target\n",
      "GET METRICS\n",
      "Original (2, 206, 3)\n",
      "EXP (2, 206, 3)\n",
      "GET METRICS\n",
      "Original (2, 206, 3)\n",
      "EXP (2, 206, 3)\n",
      "GET METRICS\n",
      "Original (2, 206, 3)\n",
      "EXP (2, 206, 3)\n"
     ]
    }
   ],
   "source": [
    "SummaryTable_faith = bm.evaluate(test_x[0:num_test_samples], np.argmax(test_y[0:num_test_samples],axis=1), model,exp=None, mode='time',aggregate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>normalize</th>\n",
       "      <th>tsr</th>\n",
       "      <th>transformer</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRAD</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FA</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>authentic_opposing_information</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method normalize   tsr                     transformer epochs\n",
       "0   GRAD      True  True                             NaN    NaN\n",
       "1     FA      True  True                             NaN    NaN\n",
       "2    NaN       NaN   NaN  authentic_opposing_information     30"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SummaryTable_faith.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SummaryTable_faith.to_csv(f\"{interp_folder}/{model_name}_faith.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The AUC metric is likely to be sensitive to the choice of ground truth mask i.e., the 's_batch' input as well as if absolute values 'abs' are taken of the attributions .  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Fawcett, Tom. 'An introduction to ROC analysis' Pattern Recognition Letters Vol 27, Issue 8, (2006).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from XTSCBench.ReliabilityEvaluation import ReliabilityEvaluation\n",
    "from quantus.metrics.localisation.auc import AUC\n",
    "\n",
    "bm=ReliabilityEvaluation(explainer=explainer,mlmodel=None, metrics=[AUC()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Target\n",
      "No Target\n"
     ]
    }
   ],
   "source": [
    "#CAREFUL THIS IS AN ASSUMPTION\n",
    "meta=np.zeros_like(test_x[0:num_test_samples])\n",
    "meta[:,10:20]= np.ones_like(meta[:,10:20])\n",
    "SummaryTable_reiable = bm.evaluate(test_x[0:num_test_samples], np.argmax(test_y[0:num_test_samples],axis=1),model,meta=meta,exp=None, mode='time',aggregate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;quantus.metrics.localisation.auc.AUC object at 0x7f3d464639d0&gt;_mean</th>\n",
       "      <th>Pointing_mean</th>\n",
       "      <th>Relevance Rank_mean</th>\n",
       "      <th>Relevance Mass_mean</th>\n",
       "      <th>AuC_mean</th>\n",
       "      <th>&lt;quantus.metrics.localisation.auc.AUC object at 0x7f3d464639d0&gt;_std</th>\n",
       "      <th>Pointing_std</th>\n",
       "      <th>Relevance Rank_std</th>\n",
       "      <th>Relevance Mass_std</th>\n",
       "      <th>AuC_std</th>\n",
       "      <th>method</th>\n",
       "      <th>normalize</th>\n",
       "      <th>tsr</th>\n",
       "      <th>transformer</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.420493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.369506e-07</td>\n",
       "      <td>0.420493</td>\n",
       "      <td>0.129997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.224092e-07</td>\n",
       "      <td>0.129997</td>\n",
       "      <td>GRAD</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.542092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.224261e-01</td>\n",
       "      <td>0.542092</td>\n",
       "      <td>0.137894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>1.702358e-01</td>\n",
       "      <td>0.137894</td>\n",
       "      <td>FA</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.698583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.767379e+00</td>\n",
       "      <td>0.698583</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.601492e+00</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>authentic_opposing_information</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   <quantus.metrics.localisation.auc.AUC object at 0x7f3d464639d0>_mean  \\\n",
       "0                                           0.420493                      \n",
       "1                                           0.542092                      \n",
       "2                                           0.698583                      \n",
       "\n",
       "   Pointing_mean  Relevance Rank_mean  Relevance Mass_mean  AuC_mean  \\\n",
       "0            0.0             0.000000         4.369506e-07  0.420493   \n",
       "1            0.0             0.166667         1.224261e-01  0.542092   \n",
       "2            0.0             0.000000         1.767379e+00  0.698583   \n",
       "\n",
       "   <quantus.metrics.localisation.auc.AUC object at 0x7f3d464639d0>_std  \\\n",
       "0                                           0.129997                     \n",
       "1                                           0.137894                     \n",
       "2                                           0.013348                     \n",
       "\n",
       "   Pointing_std  Relevance Rank_std  Relevance Mass_std   AuC_std method  \\\n",
       "0           0.0            0.000000        2.224092e-07  0.129997   GRAD   \n",
       "1           0.0            0.235702        1.702358e-01  0.137894     FA   \n",
       "2           0.0            0.000000        2.601492e+00  0.013348    NaN   \n",
       "\n",
       "  normalize   tsr                     transformer epochs  \n",
       "0      True  True                             NaN    NaN  \n",
       "1      True  True                             NaN    NaN  \n",
       "2       NaN   NaN  authentic_opposing_information     30  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SummaryTable_reiable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SummaryTable_reiable.to_csv(f\"{interp_folder}/{model_name}_reliable.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Effective Complexity metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs' and the choice of threshold 'eps'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Nguyen, An-phi, and María Rodríguez Martínez. 'On quantitative aspects of model interpretability.' arXiv preprint arXiv:2007.07584 (2020)..\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from XTSCBench.ComplexityEvaluation import ComplexityEvaluation\n",
    "from quantus.metrics.complexity.effective_complexity import EffectiveComplexity\n",
    "\n",
    "bm=ComplexityEvaluation(explainer=explainer, metrics= [EffectiveComplexity()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Target\n",
      "No Target\n"
     ]
    }
   ],
   "source": [
    "SummaryTable_complex = bm.evaluate(test_x[0:num_test_samples], np.argmax(test_y[0:num_test_samples],axis=1), model, mode='time',aggregate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity_mean</th>\n",
       "      <th>&lt;quantus.metrics.complexity.effective_complexity.EffectiveComplexity object at 0x7f3d4616ba90&gt;_mean</th>\n",
       "      <th>complexity_std</th>\n",
       "      <th>&lt;quantus.metrics.complexity.effective_complexity.EffectiveComplexity object at 0x7f3d4616ba90&gt;_std</th>\n",
       "      <th>method</th>\n",
       "      <th>normalize</th>\n",
       "      <th>tsr</th>\n",
       "      <th>transformer</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.377835</td>\n",
       "      <td>158.5</td>\n",
       "      <td>0.259202</td>\n",
       "      <td>30.405592</td>\n",
       "      <td>GRAD</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.392748</td>\n",
       "      <td>519.0</td>\n",
       "      <td>0.580744</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>FA</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.155328</td>\n",
       "      <td>614.0</td>\n",
       "      <td>0.065276</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>authentic_opposing_information</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   complexity_mean  \\\n",
       "0         3.377835   \n",
       "1         4.392748   \n",
       "2         6.155328   \n",
       "\n",
       "   <quantus.metrics.complexity.effective_complexity.EffectiveComplexity object at 0x7f3d4616ba90>_mean  \\\n",
       "0                                              158.5                                                     \n",
       "1                                              519.0                                                     \n",
       "2                                              614.0                                                     \n",
       "\n",
       "   complexity_std  \\\n",
       "0        0.259202   \n",
       "1        0.580744   \n",
       "2        0.065276   \n",
       "\n",
       "   <quantus.metrics.complexity.effective_complexity.EffectiveComplexity object at 0x7f3d4616ba90>_std  \\\n",
       "0                                          30.405592                                                    \n",
       "1                                           4.242641                                                    \n",
       "2                                           4.242641                                                    \n",
       "\n",
       "  method normalize   tsr                     transformer epochs  \n",
       "0   GRAD      True  True                             NaN    NaN  \n",
       "1     FA      True  True                             NaN    NaN  \n",
       "2    NaN       NaN   NaN  authentic_opposing_information     30  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SummaryTable_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "SummaryTable_complex.to_csv(f\"{interp_folder}/{model_name}_complexity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XTSCBench.RobustnessEvaluation import RobustnessEvaluation\n",
    "\n",
    "bm=RobustnessEvaluation(explainer=explainer,mlmodel=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Target\n",
      "No Target\n",
      "Robustness Shapes\n",
      "(2, 96, 1)\n",
      "[1, 1]\n",
      "X1  (1, 96)\n",
      "y1  1\n",
      "No Target\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m SummaryTable_robust \u001b[38;5;241m=\u001b[39m \u001b[43mbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43maggregate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/D_drive/UCSD/Quarters/Q4/DSC261-RespDS/ts_causal_wsl/lib/python3.11/site-packages/XTSCBench/RobustnessEvaluation.py:52\u001b[0m, in \u001b[0;36mRobustnessEvaluation.evaluate\u001b[0;34m(self, items, label, model, exp, explainer, mode, aggregate)\u001b[0m\n\u001b[1;32m     50\u001b[0m label \u001b[38;5;241m=\u001b[39m get_preds(model, items)\n\u001b[1;32m     51\u001b[0m res\u001b[38;5;241m=\u001b[39mget_explanation(items, label, data_shape_1, data_shape_2, baseline, model,mode)\n\u001b[0;32m---> 52\u001b[0m row_summary\u001b[38;5;241m=\u001b[39m\u001b[43mget_robustness_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m aggregate:\n\u001b[1;32m     54\u001b[0m     df\u001b[38;5;241m=\u001b[39mparameters_to_pandas(baseline)\n",
      "File \u001b[0;32m/mnt/c/D_drive/UCSD/Quarters/Q4/DSC261-RespDS/ts_causal_wsl/lib/python3.11/site-packages/XTSCBench/metrics/robustness_metrics.py:61\u001b[0m, in \u001b[0;36mget_robustness_metrics\u001b[0;34m(original, exp, mlmodel, labels, explainer, mode, additional_metrics)\u001b[0m\n\u001b[1;32m     59\u001b[0m explainer\u001b[38;5;241m=\u001b[39mQuantus_Wrapper(explainer, mode)\u001b[38;5;241m.\u001b[39mmake_callable\n\u001b[1;32m     60\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([])\n\u001b[0;32m---> 61\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverageSensitivity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mAverageSensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexplainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchannel_first\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     62\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxSensitivity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(MaxSensitivity(mlmodel,original,labels, exp,explainer,channel_first))\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m additional_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n",
      "File \u001b[0;32m/mnt/c/D_drive/UCSD/Quarters/Q4/DSC261-RespDS/ts_causal_wsl/lib/python3.11/site-packages/XTSCBench/metrics/robustness_metrics.py:27\u001b[0m, in \u001b[0;36mAverageSensitivity\u001b[0;34m(mod, data, label, res, exp, channel_first)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03mmeasures the average sensitivity of an explanation using a Monte Carlo sampling-based approximation \u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     25\u001b[0m metric \u001b[38;5;241m=\u001b[39m quantus\u001b[38;5;241m.\u001b[39mAvgSensitivity(nr_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,lower_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, perturb_func\u001b[38;5;241m=\u001b[39mquantus\u001b[38;5;241m.\u001b[39muniform_noise, similarity_func\u001b[38;5;241m=\u001b[39mquantus\u001b[38;5;241m.\u001b[39mdifference,disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mexplain_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchannel_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannel_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m/mnt/c/D_drive/UCSD/Quarters/Q4/DSC261-RespDS/ts_causal_wsl/lib/python3.11/site-packages/quantus/metrics/robustness/avg_sensitivity.py:254\u001b[0m, in \u001b[0;36mAvgSensitivity.__call__\u001b[0;34m(self, model, x_batch, y_batch, a_batch, s_batch, channel_first, explain_func, explain_func_kwargs, model_predict_kwargs, softmax, device, batch_size, custom_batch, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    166\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    This implementation represents the main logic of the metric and makes the class object callable.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    It completes instance-wise evaluation of explanations (a_batch) with respect to input data (x_batch),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m        >> scores = metric(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch_saliency}\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43ma_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchannel_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannel_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplain_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplain_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplain_func_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplain_func_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_predict_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_predict_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/D_drive/UCSD/Quarters/Q4/DSC261-RespDS/ts_causal_wsl/lib/python3.11/site-packages/quantus/metrics/base_batched.py:217\u001b[0m, in \u001b[0;36mBatchedMetric.__call__\u001b[0;34m(self, model, x_batch, y_batch, a_batch, s_batch, channel_first, explain_func, explain_func_kwargs, model_predict_kwargs, softmax, device, batch_size, custom_batch, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_batch \u001b[38;5;129;01min\u001b[39;00m batch_generator:\n\u001b[0;32m--> 217\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_results\u001b[38;5;241m.\u001b[39mextend(result)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Call post-processing.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/D_drive/UCSD/Quarters/Q4/DSC261-RespDS/ts_causal_wsl/lib/python3.11/site-packages/quantus/metrics/robustness/avg_sensitivity.py:337\u001b[0m, in \u001b[0;36mAvgSensitivity.evaluate_batch\u001b[0;34m(self, model, x_batch, y_batch, a_batch, s_batch)\u001b[0m\n\u001b[1;32m    331\u001b[0m     warn\u001b[38;5;241m.\u001b[39mwarn_perturbation_caused_no_change(\n\u001b[1;32m    332\u001b[0m         x\u001b[38;5;241m=\u001b[39mx_instance,\n\u001b[1;32m    333\u001b[0m         x_perturbed\u001b[38;5;241m=\u001b[39mx_instance_perturbed,\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Generate explanation based on perturbed input x.\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m a_perturbed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_func_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalise:\n\u001b[1;32m    345\u001b[0m     a_perturbed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalise_func(\n\u001b[1;32m    346\u001b[0m         a_perturbed,\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalise_func_kwargs,\n\u001b[1;32m    348\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/c/D_drive/UCSD/Quarters/Q4/DSC261-RespDS/ts_causal_wsl/lib/python3.11/site-packages/XTSCBench/metrics/metrics_helper.py:62\u001b[0m, in \u001b[0;36mQuantus_Wrapper.make_callable\u001b[0;34m(self, model, inputs, targets, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my1 \u001b[39m\u001b[38;5;124m'\u001b[39m, y1)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     a\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(a)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     64\u001b[0m         a,b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer\u001b[38;5;241m.\u001b[39mexplain(x1\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28mint\u001b[39m(y1))\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "SummaryTable_robust = bm.evaluate(test_x[0:2], np.argmax(test_y[0:2],axis=1), model,exp=None, mode='time',aggregate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SummaryTable_robust.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_causal_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
